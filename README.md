# ğŸ” Taiwan's Fastfood History RAG Assistant

An intelligent **Retrieval-Augmented Generation (RAG)** assistant that helps users explore the **history and cultural evolution of fast food in Taiwan**, powered by both **OpenAI GPT-4o** and **Meta LLaMA3-8B**, combining the strength of multiple embedding models, ChromaDB, and cross-encoder reranking.

---

## âœ¨ Key Capabilities

- ğŸ” **Semantic Search**: Retrieve relevant chunks from a PDF-based knowledge base using **multilingual sentence embeddings**.
- ğŸ§  **Dual LLM Support**: Choose between **GPT-4o** and **LLaMA3.1:8B** to generate grounded answers.
- âš–ï¸ **Two-Step Retrieval Pipeline**:
  - **Bi-encoder** for efficient similarity-based initial retrieval
  - **Cross-encoder reranker** (`BAAI/bge-reranker-base`) for improved relevance scoring
- ğŸ—ƒï¸ **Knowledge Source**: Local PDF documents about fast food and McDonald's development in Taiwan
- ğŸˆ¶ **Traditional Chinese Support** using `intfloat/multilingual-e5-large` for embedding and query understanding

---

## ğŸ§© Stack

- ğŸ“š **LangChain** â€“ document parsing and chunking
- ğŸ“¦ **ChromaDB** â€“ lightweight vector database for fast similarity search
- ğŸ§  **SentenceTransformers** â€“ `intfloat/multilingual-e5-large` for chunk embeddings
- ğŸ” **Hugging Face Transformers** â€“ `BAAI/bge-reranker-base` for cross-encoder reranking
- ğŸ¤– **LLMs**:
  - `gpt-4o` via OpenAI API
  - `LLaMA3-8B-Instruct` (local inference or via API)

---

## ğŸ“¦ Installation

Clone the repository:

```bash
git clone https://github.com/your-username/McDonaldsHistory-Agent.git
cd McDonaldsHistory-Agent
```

## ğŸ›  Setup Instructions
Create a Virtual Environment and Install Dependenciesï¼š
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```
## ğŸ” Create a .env File
In the root directory, add your OpenAI API key:
``` bash
OPENAI_API_KEY=your-openai-api-key
```
## ğŸ’¡ How It Works
1. Load and split your PDF documents (related to McDonaldâ€™s) into smaller chunks.

2. Store chunks in ChromaDB using multilingual embeddings.

3. On user query:
   - Retrieve top 5 candidates using vector similarity. 
   - Rerank them using a cross-encoder reranker. 
   - Use the best-matching chunk as context for GPT-4o to generate the final answer.
## ğŸ§ª Example Query
``` bash
"è«‹å•éº¥ç•¶å‹ä»€éº¼æ™‚å€™åœ¨å°ç£é–‹äº†ç¬¬ä¸€é–“åº—ï¼Ÿ
 ----------------
 Adam çš„å›ç­”ï¼ˆæ ¹æ“šæ–‡ä»¶å…§å®¹ï¼‰ï¼š
ã€Œæ ¹æ“šè³‡æ–™ï¼Œéº¥ç•¶å‹åœ¨1984å¹´é¦–æ¬¡é€²å…¥å°ç£å¸‚å ´ï¼Œç¬¬ä¸€å®¶åº—ä½æ–¼å°åŒ—å¸‚ã€‚ã€"
```